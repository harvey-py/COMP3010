{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWNvAif9LqB4"
   },
   "source": [
    "# **COMP3010 - Machine Learning**\n",
    "\n",
    "The tutorial contains two parts: (theoretical) discussion and (practical) coding. The discussion part consists of important concepts, advanced topics, or open-ended questions, for which we want an in-depth discussion. The coding part contains programming exercises for you to gain hands on experience.\n",
    "\n",
    "## **Tutorial 04**\n",
    "Learning outcomes:\n",
    "\n",
    "*   Implement and apply decision trees\n",
    "*   Implement cross-validation for hyperparameter tuning\n",
    "*   Apply ensemble tree methods, such as Random Forests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iujOf7090at7"
   },
   "source": [
    "## An example tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "executionInfo": {
     "elapsed": 701,
     "status": "ok",
     "timestamp": 1705036941726,
     "user": {
      "displayName": "QILIN LI",
      "userId": "01132023268323719979"
     },
     "user_tz": -480
    },
    "id": "0-6vqRvBHsJd",
    "outputId": "91c194a9-7fbe-4862-ba75-4ebdc260ee72"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgraphviz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Digraph\n\u001b[1;32m      3\u001b[0m styles \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mellipse\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstyle\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilled\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlightblue\u001b[39m\u001b[38;5;124m'\u001b[39m},\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m'\u001b[39m:  {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcircle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstyle\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilled\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m},\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcircle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstyle\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilled\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlightgreen\u001b[39m\u001b[38;5;124m'\u001b[39m},\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqst\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrect\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m      8\u001b[0m }\n\u001b[1;32m     10\u001b[0m example_tree \u001b[38;5;241m=\u001b[39m Digraph()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'graphviz'"
     ]
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "styles = {\n",
    "    'top': {'shape': 'ellipse', 'style': 'filled', 'color': 'lightblue'},\n",
    "    'no':  {'shape': 'circle', 'style': 'filled', 'color': 'red'},\n",
    "    'yes': {'shape': 'circle', 'style': 'filled', 'color': 'lightgreen'},\n",
    "    'qst': {'shape': 'rect'}\n",
    "}\n",
    "\n",
    "example_tree = Digraph()\n",
    "\n",
    "example_tree.node('top', 'Should I attend the ML lecture?', styles['top'])\n",
    "example_tree.node('q1', 'Do I fulfill requirements?', styles['qst'])\n",
    "\n",
    "example_tree.node('q2', 'Do I like CS?', styles['qst'])\n",
    "example_tree.node('no1', 'No ', styles['no'])\n",
    "\n",
    "example_tree.node('q3', 'Is the lecture early in the morning?', styles['qst'])\n",
    "example_tree.node('no2', 'No ', styles['no'])\n",
    "\n",
    "example_tree.node('no3', 'No ', styles['no'])\n",
    "example_tree.node('yes', 'Yes', styles['yes'])\n",
    "\n",
    "example_tree.edge('top', 'q1')\n",
    "\n",
    "example_tree.edge('q1', 'q2', 'Yes')\n",
    "example_tree.edge('q1', 'no1', 'No')\n",
    "\n",
    "example_tree.edge('q2', 'q3', 'Yes')\n",
    "example_tree.edge('q2', 'no2', 'No')\n",
    "\n",
    "example_tree.edge('q3', 'no3', 'Yes')\n",
    "example_tree.edge('q3', 'yes', 'No')\n",
    "\n",
    "example_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3BdVmSk1QFu"
   },
   "source": [
    "## **Discussion**\n",
    "\n",
    "\n",
    "1.   Describe how a decision tree is built.\n",
    "\n",
    "2.   Compare and contrast decision tree classifers with kNN classifiers.\n",
    "\n",
    "3.   How can pruning be used to address overfitting in decision trees? Explain the difference between pre-pruning and post-pruning.\n",
    "\n",
    "4.   How would you approach the task of selecting between a single decision tree, random forest, and GBDT for a given problem.\n",
    "\n",
    "5.   Compare the bias-variance tradeoff in various machine learning algorithms (e.g., K-Nearest Neighbors, SVM, Decision Trees, Neural Networks). Which algorithms are typically high bias and low variance and vice versa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5oGmB1yK-qIS"
   },
   "source": [
    "## **Coding**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSdI2ILLknQB"
   },
   "source": [
    "In this part, you will implement logsitic regression and SVM for a classification problem.\n",
    "\n",
    "Part 2 is adapted from [Machine Learning Specialisation](https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera) by Andrew Ng.\n",
    "\n",
    "Part 3 is adapted from [Scikit-learn: SVM](https://scikit-learn.org/stable/auto_examples/svm/plot_svm_kernels.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dl5c4w3iVrrc"
   },
   "source": [
    "## Outline\n",
    "- [ 1 - Information theory recap ](#1)\n",
    "- [ 2 - Build a decision tree (ID3) step-by-step ](#2)\n",
    "  - [ 2.1 - Exercise01: Select the first attribute to split](#2.1)\n",
    "  - [ 2.2 - Split the first attribute](#2.2)\n",
    "  - [ 2.3 - Split the next branch](#2.3)\n",
    "  - [ 2.4 - Summary](#2.4)\n",
    "- [ 3 - Ensemble learning](#3)\n",
    "  - [3.1 - Decision tree with varing depths](#3.1)\n",
    "  - [3.2 - Exercise02: Random forest with varing depths](#3.2)\n",
    "- [ 4 - Hyper-parameter tuning by cross-validation](#4)\n",
    "  - [4.1 Exercise03: hyperparameter tuning](#4.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZEVAQ8GNzPzm"
   },
   "source": [
    "<a name=\"1\"></a>\n",
    "## 1 - Information theorey recap\n",
    "\n",
    "First, let's review the concept of information theorey, especially the information gain.\n",
    "* Let $T$ be the set of training samples with $n$ possible outcomes, thus $T = \\{T_1, T_2, ..., T_n\\}$\n",
    "\n",
    "* The entropy is given by <p align=\"center\"><br>$H(T) = -\\sum\\limits_{i=1}^N p_i\\cdot\\log(p_i) = -\\sum\\limits_{i=1}^N \\frac{|T_i|}{|T|}\\cdot\\log(\\frac{|T_i|}{|T|})$</p><br>\n",
    "    \n",
    "* We can also calulate the entropy after $T$ was partitioned in $T_i$ with respect to some feature $X$ <p align=\"center\"><br>$H(T, X) = \\sum\\limits_{i=1}^N p_i\\cdot H(T_i)$</p>\n",
    "\n",
    "* And the information gain is defined as <p align=\"center\"><br>$IG(X) = H(T) - H(T, X)$</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hP1SlGEJ-oQ"
   },
   "source": [
    "1.1 - Toy example\n",
    "\n",
    "Let's go through a toy example step by step. Consider a toy dataset with\n",
    "*   9 examples,\n",
    "*   3 features X1, X2, X3,\n",
    "*   2 classes C01, C02\n",
    "\n",
    "```\n",
    "   X1  ||  A  |  A  |  A  |  B  |  B  |  C  |  C  |  C  |  C  |\n",
    "---------------------------------------------------------------\n",
    "   X2  ||  0  |  0  |  1  |  1  |  0  |  1  |  1  |  1  |  0  |\n",
    "---------------------------------------------------------------\n",
    "   X3  || RED | GRN | GRN | BLU | RED | GRN | BLU | RED | GRN |\n",
    "===============================================================\n",
    " Class || C01 | C01 | C02 | C02 | C02 | C02 | C01 | C01 | C02 |\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHvOabNUMhQN"
   },
   "source": [
    "* The *root* entropy\n",
    "\n",
    "    * We have 9 samples: 4 belong to class C01 and 5 to C02 <p align=\"center\"><br>$H(T) = -\\frac{4}{9}\\log(\\frac{4}{9}) - \\frac{5}{9}\\log(\\frac{5}{9}) = 0.99$</p><br>\n",
    "    \n",
    "* Now lets consider feature X1, which splits data into subsets $T_1$, $T_2$, and $T_3$ (with X1 value A, B, and C, respectively)<br><br>\n",
    "\n",
    "    * Within $T_1$ there are 3 samples: 2 from C01 and 1 from C02 <p align=\"center\"><br>$H(T_1) = -\\frac{2}{3}\\log(\\frac{2}{3}) - \\frac{1}{3}\\log(\\frac{1}{3}) = 0.92$</p><br>\n",
    "    \n",
    "    * Within $T_2$ there are 2 samples: 0 from C01 and 2 from C02 <p align=\"center\"><br>$H(T_2) = -\\frac{2}{2}\\log(\\frac{2}{2}) - \\frac{0}{2}\\log(\\frac{0}{2}) = 0.00$</p><br>\n",
    "    \n",
    "    * Within $T_3$ there are 4 samples: 2 from C01 and 2 from C02 <p align=\"center\"><br>$H(T_3) = -\\frac{2}{4}\\log(\\frac{2}{4}) - \\frac{2}{4}\\log(\\frac{2}{4}) = 1.00$</p><br>\n",
    "    \n",
    "    * The resulting entropy is <p align=\"center\"><br>$H(T, X1) = \\frac{3}{9}\\cdot H(T_1) + \\frac{2}{9}\\cdot H(T_2) + \\frac{4}{9}\\cdot H(T_3) = 0.75$</p><br>\n",
    "    \n",
    "    * Thus, infromation gain if the set is split according to X1 <p align=\"center\"><br>$IG(X1) = H(T) - H(T, X1) = 0.99 - 0.75 = 0.24 \\text{ Sh }$</p><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JlLx8Pe9NAm2"
   },
   "source": [
    "We write a helper fucntion to do the above computation, in which the `probs` contains the counts $T_i$ for each outcome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1705038919138,
     "user": {
      "displayName": "QILIN LI",
      "userId": "01132023268323719979"
     },
     "user_tz": -480
    },
    "id": "aYZJbuZkL4SL",
    "outputId": "fd332124-4ebe-4690-a057-05f5979a8d8a"
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def entropy(*probs):\n",
    "  \"\"\"Calculate information entropy\"\"\"\n",
    "  try:\n",
    "    total = sum(probs)\n",
    "    return sum([-p / total * log(p / total, 2) for p in probs])\n",
    "  except:\n",
    "    return 0\n",
    "\n",
    "H_T = entropy(4, 5)\n",
    "H_T1 = entropy(2, 1)\n",
    "H_T2 = entropy(0, 2)\n",
    "H_T3 = entropy(2, 2)\n",
    "H_T_X1 = 3/9*H_T1 + 2/9*H_T2 + 4/9*H_T3\n",
    "IG_X1 = H_T - H_T_X1\n",
    "\n",
    "print(\"The root entropy is: \", H_T)\n",
    "print(\"The child entropy of subset T1 is: \", H_T1)\n",
    "print(\"The child entropy of subset T2 is: \", H_T2)\n",
    "print(\"The child entropy of subset T3 is: \", H_T3)\n",
    "print(\"The (weighted) average entropy after splliting according to X1 is: \", H_T_X1)\n",
    "print(\"The information gain of splitting based on X1 is: \", IG_X1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtCWAYIK27lj"
   },
   "source": [
    "<a name=\"2\"></a>\n",
    "## 2 - Build a decision tree (ID3) step-by-step\n",
    "\n",
    "In this part, we will build a decision tree based on the ID3 algorithm.  We will use the popular `Play Golf` dataset, which contains:\n",
    "*   4 features:\n",
    "    *   outlook: rainy, overcast, sunny\n",
    "    *   temperature: cool, mid, hot\n",
    "    *   humidity: normal, high\n",
    "    *   windy: false, true\n",
    "*   2 classes (play golf?):\n",
    "    *   true\n",
    "    *   false\n",
    "\n",
    "Let's load the data first:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "executionInfo": {
     "elapsed": 314,
     "status": "ok",
     "timestamp": 1705041143723,
     "user": {
      "displayName": "QILIN LI",
      "userId": "01132023268323719979"
     },
     "user_tz": -480
    },
    "id": "3gOf8REjm54N",
    "outputId": "b2d67e47-ce51-496f-b06e-a1fed0b0a9eb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# first row = headers\n",
    "src = \"https://gist.github.com/kudaliar032/b8cf65d84b73903257ed603f6c1a2508/raw/aa4826eb745d09a2df4967a793410e141642cbed/golf-dataset.csv\"\n",
    "\n",
    "golf_data = pd.read_csv(src)\n",
    "golf_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFyv31jw4xYD"
   },
   "source": [
    "<a name=\"2.1\"></a>\n",
    "### 2.1 Exercise1 - Select the first attribute to split\n",
    "\n",
    "Starting from an empty tree, the first step is to select the first attribute to split, and the criteria is to maximise the information gain. Therefore, we need to compute 1) the root entropy; 2) for each feature, compute resulting entropy after splitting and the difference with 1; 3) find the feature that gives the largest information gain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 302,
     "status": "ok",
     "timestamp": 1705042156169,
     "user": {
      "displayName": "QILIN LI",
      "userId": "01132023268323719979"
     },
     "user_tz": -480
    },
    "id": "Heu9RVs64ynT"
   },
   "outputs": [],
   "source": [
    "# 1) compute the root entropy H_T\n",
    "\n",
    "# 2.1) Outlook: compute entropy H_outlook and IG_outlook\n",
    "\n",
    "# 2.2) Temperature: compute entropy H_temp and IG_temp\n",
    "\n",
    "# 2.3) Humidity: compute entropy H_humi and IG_humi\n",
    "\n",
    "# 2.4) Windy: compute entropy H_windy and IG_windy\n",
    "\n",
    "# 3) identify the largest IG?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3vsgGIk42Sp"
   },
   "source": [
    "**Expected Output**:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td> <b>The root entropy H(T)= 0.94<b></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> <b>The information gain (IG) for outlook IG(outlook)= 0.25<b></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> <b>The IG for temperature IG(temp)= 0.03<b></td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td> <b>The IG for humidity IG(humi)= 0.15<b></td>\n",
    "  </tr>\n",
    "      <tr>\n",
    "    <td> <b>The IG for windy IG(humi)= 0.05<b></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sIoBredUfSbc"
   },
   "source": [
    "<a name=\"2.2\"></a>\n",
    "### 2.2 Split the first attribute\n",
    "\n",
    "`Outlook` has the largest information gain and thus we split based on this feature. Note that a branch with __zero entropy__ is a leaf node: overcast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "executionInfo": {
     "elapsed": 381,
     "status": "ok",
     "timestamp": 1705043193796,
     "user": {
      "displayName": "QILIN LI",
      "userId": "01132023268323719979"
     },
     "user_tz": -480
    },
    "id": "4MzxLWab43le",
    "outputId": "ed1e5c57-6541-47e4-aaad-a531dfca4011"
   },
   "outputs": [],
   "source": [
    "tree = Digraph()\n",
    "\n",
    "tree.edge(\"outlook\", \"sunny\")\n",
    "tree.edge(\"outlook\", \"overcast\")\n",
    "tree.edge(\"outlook\", \"rainy\")\n",
    "\n",
    "tree.edge(\"overcast\", \"yes\")\n",
    "\n",
    "tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMCD66iX5A6S"
   },
   "source": [
    "<a name=\"2.3\"></a>\n",
    "### 2.3 Split the next branch\n",
    "\n",
    "After that, we need to split the next branch. Following the left-to-right convention, it means ``outlook=sunny``. Let's first filter out this subset of data examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 320,
     "status": "ok",
     "timestamp": 1705043428684,
     "user": {
      "displayName": "QILIN LI",
      "userId": "01132023268323719979"
     },
     "user_tz": -480
    },
    "id": "_5XLopJ15Dgq",
    "outputId": "29def1af-f55f-42ae-85b6-3479004de765"
   },
   "outputs": [],
   "source": [
    "golf_data.loc[golf_data['Outlook'] == \"Sunny\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xIOTHXygmuX"
   },
   "source": [
    "We will then recursively find the \"best\" feature to split based on the information gain. We will skip the calculation for now, and in this case, you will find ``windy`` has the largest information gain, which will be taken for this split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464
    },
    "executionInfo": {
     "elapsed": 347,
     "status": "ok",
     "timestamp": 1705043607310,
     "user": {
      "displayName": "QILIN LI",
      "userId": "01132023268323719979"
     },
     "user_tz": -480
    },
    "id": "Yrh-jpJghGIn",
    "outputId": "293cd5a4-acbd-4e53-f7bf-c945fbd30366"
   },
   "outputs": [],
   "source": [
    "tree.edge(\"sunny\", \"windy\")\n",
    "\n",
    "tree.edge(\"windy\", \"false\")\n",
    "tree.edge(\"windy\", \"true\")\n",
    "\n",
    "tree.edge(\"false\", \"yes\")\n",
    "tree.edge(\"true\", \"no\")\n",
    "\n",
    "tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "en9DziUJhUYv"
   },
   "source": [
    "Following this, we will split the last branch ``outlook==rainy``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 384,
     "status": "ok",
     "timestamp": 1705043697698,
     "user": {
      "displayName": "QILIN LI",
      "userId": "01132023268323719979"
     },
     "user_tz": -480
    },
    "id": "DH84EzJohg0A",
    "outputId": "71fbb8a8-41cf-4bcd-c56b-f3b98182c965"
   },
   "outputs": [],
   "source": [
    "golf_data.loc[golf_data['Outlook'] == \"Rainy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8ZkohiAhiwi"
   },
   "source": [
    "In this case, ``humidity`` will has the largest information gain ( as can be seen from its correspondence with the target ``play golf``). Therefore, ``humidity`` is used for split ater ``rainy``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464
    },
    "executionInfo": {
     "elapsed": 392,
     "status": "ok",
     "timestamp": 1705043805517,
     "user": {
      "displayName": "QILIN LI",
      "userId": "01132023268323719979"
     },
     "user_tz": -480
    },
    "id": "0guUl0smh72Z",
    "outputId": "8e639ae7-1bf4-4db5-f073-453f8b96ea44"
   },
   "outputs": [],
   "source": [
    "tree.edge(\"rainy\", \"humidity\")\n",
    "\n",
    "tree.edge(\"humidity\", \"high\")\n",
    "tree.edge(\"humidity\", \"normal\")\n",
    "\n",
    "tree.edge(\"normal\", \"yes\")\n",
    "tree.edge(\"high\", \"no \")\n",
    "\n",
    "tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wd-llUHAiANW"
   },
   "source": [
    "<a name=\"2.4\"></a>\n",
    "### 2.4 Summary\n",
    "\n",
    "Congradulations! You have just built the entire decision tree for the Play Golf dataset using ID3 algorithm. Several notes:\n",
    "*   Note that we didn't even end up with using the temperature attribute. Why?\n",
    "*   The main problem with ID3 is overfitting as the tree doesn't stop growing until the whole training set is classified. Need prunning!\n",
    "*   There is a bias towards features with many possible outcomes. Why?\n",
    "*   There are other tree construction methods, e.g., C4.5, CART, which improves ID3 with additional techniques, such as information gain normalization, prunning, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vp_MIX4JWmB"
   },
   "source": [
    "<a name=\"3\"></a>\n",
    "## 3 - Ensemble Learning\n",
    "\n",
    "In this exercise, we will conduct ensemble learning, in particular, ensembled tree methods, using scikit-learn (Hoo...ray!). We will show that a single decision tree can easily overfit a given dataset with enough tree depth, while the Random forest algorithm mitigates the problem with bagging.\n",
    "\n",
    "Let's first generate data by making blobs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "executionInfo": {
     "elapsed": 1328,
     "status": "ok",
     "timestamp": 1705045323384,
     "user": {
      "displayName": "QILIN LI",
      "userId": "01132023268323719979"
     },
     "user_tz": -480
    },
    "id": "P6Fb8VaSnUML",
    "outputId": "b6211eb1-930f-48fa-c2b5-822aca109b10"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# generate 5 blobs with fixed random generator\n",
    "X, Y = make_blobs(n_samples=500, centers=8, random_state=300)\n",
    "\n",
    "plt.scatter(*X.T, c=Y, marker='.', cmap='Dark2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GX-Kz2XvnfOw"
   },
   "source": [
    "To make our life easier we create a helper function to:\n",
    "* plot training data on existing axes or new one (if not provided)\n",
    "\n",
    "* train given classifier on given dataset\n",
    "\n",
    "* create countours representing predictions of the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1705045325000,
     "user": {
      "displayName": "QILIN LI",
      "userId": "01132023268323719979"
     },
     "user_tz": -480
    },
    "id": "2FA3X2uFnexK"
   },
   "outputs": [],
   "source": [
    "def train_and_look(classifier, X, Y, ax=None, title='', cmap='Dark2'):\n",
    "  \"\"\"Train classifier on (X,Y). Plot data and prediction.\"\"\"\n",
    "  # create new axis if not provided\n",
    "  ax = ax or plt.gca();\n",
    "\n",
    "  ax.set_title(title)\n",
    "\n",
    "  # plot training data\n",
    "  ax.scatter(*X.T, c=Y, marker='.', cmap=cmap)\n",
    "\n",
    "  # train a cliassifier\n",
    "  classifier.fit(X, Y)\n",
    "\n",
    "  # create a grid of testing points\n",
    "  x_, y_ = np.meshgrid(np.linspace(*ax.get_xlim(), num=200),\n",
    "                       np.linspace(*ax.get_ylim(), num=200))\n",
    "\n",
    "  # convert to an array of 2D points\n",
    "  test_data = np.vstack([x_.ravel(), y_.ravel()]).T\n",
    "\n",
    "  # make a prediction and reshape to grid structure\n",
    "  z_ = classifier.predict(test_data).reshape(x_.shape)\n",
    "\n",
    "  # arange z bins so class labels are in the middle\n",
    "  z_levels = np.arange(len(np.unique(Y)) + 1) - 0.5\n",
    "\n",
    "  # plot contours corresponding to classifier prediction\n",
    "  ax.contourf(x_, y_, z_, alpha=0.25, cmap=cmap, levels=z_levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yrfeF4-lnxI2"
   },
   "source": [
    "Let's check how it works on a decision tree classifier with default settings from sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "executionInfo": {
     "elapsed": 1042,
     "status": "ok",
     "timestamp": 1705045368953,
     "user": {
      "displayName": "QILIN LI",
      "userId": "01132023268323719979"
     },
     "user_tz": -480
    },
    "id": "QyHPAuzsn2xy",
    "outputId": "c2000ac1-ce33-489d-a7c3-11d29f86ad63"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as DT\n",
    "\n",
    "train_and_look(DT(), X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGQe4JcTs5wL"
   },
   "source": [
    "Note that sklearn also provided a function [plot_tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html#sklearn.tree.plot_tree) to visualize the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FGTQu4QoHQG"
   },
   "source": [
    "<a name=\"3.1\"></a>\n",
    "### 3.1 - Decision tree with varing depths\n",
    "\n",
    "Now we're ready to do some real business. We will consider decision trees of varing capacity by changing one of the important hyperparameter maximum depths from 1 to 9, and see how they classify the blob dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 853
    },
    "executionInfo": {
     "elapsed": 4514,
     "status": "ok",
     "timestamp": 1705045665434,
     "user": {
      "displayName": "QILIN LI",
      "userId": "01132023268323719979"
     },
     "user_tz": -480
    },
    "id": "kZ30rY6aoud_",
    "outputId": "c1b51012-a540-4bb1-e9f6-570ebe3116f4"
   },
   "outputs": [],
   "source": [
    "# create a figure with 9 axes 3x3\n",
    "fig, ax = plt.subplots(3, 3, figsize=(12,10))\n",
    "\n",
    "# train and look at decision trees with different max depth\n",
    "for max_depth in range(0, 9):\n",
    "  train_and_look(DT(max_depth=max_depth + 1), X, Y,\n",
    "                 ax=ax[max_depth // 3][max_depth % 3],\n",
    "                 title=\"Max depth = {}\".format(max_depth + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-7-x8bspGR6"
   },
   "source": [
    "What do you see?\n",
    "\n",
    "\n",
    "*   ``max_depth`` <= 3  -> undefitting\n",
    "*   4 <=``max_depth`` <= 6  -> fit well\n",
    "*   ``max_depth`` > 6  ->  overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7U1hcG2pqNZ"
   },
   "source": [
    "<a name=\"3.2\"></a>\n",
    "### 3.2 - Exercise02: Random forest with varing depths\n",
    "\n",
    "Let's do the same with random forests. Please check the [random forests doc](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voFqva1QqwwT"
   },
   "source": [
    "What do you see this time?\n",
    "\n",
    "\n",
    "*   The combination of shallow trees does a good job\n",
    "*   Overfitting of deep tree is somewhat prevented\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RcsIo5wXvJ8c"
   },
   "source": [
    "<a name=\"4\"></a>\n",
    "## 4 - Hyper-parameter tuning by cross-validation\n",
    "\n",
    "We already know that decision tree can easily overfit data, because without any stopping criterion the tree is growing until there is a sinple point in a leaf (for numerical features). One technique is to use __pre-prunning or early stopping__ by setting up:\n",
    "\n",
    "*   a max depth\n",
    "*   a min number of examples in a leaf\n",
    "*   a min information gain\n",
    "*   ...\n",
    "\n",
    "These are all ``hyperparameters`` of decision trees. How do we know what values to use for these hyperparameters? -- use validation set. Here we will go through one validation technique called cross-validation.\n",
    "\n",
    "Again, to make it easiler, we have prepared a helper class to do the following:\n",
    "\n",
    "\n",
    "*   It takes training data and hyperparameter name (as named in [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier))\n",
    "*   It can change hyperparameter\n",
    "*   It can perform a cross-validation for a set of hyperparameter values\n",
    "*   It can make accuracy and best fit plots\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 297,
     "status": "ok",
     "timestamp": 1705048202339,
     "user": {
      "displayName": "QILIN LI",
      "userId": "01132023268323719979"
     },
     "user_tz": -480
    },
    "id": "D76VTNXpyN_E"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "class TreeCV:\n",
    "  \"\"\"Perform a cross-validation for chosen hyperparameter\"\"\"\n",
    "\n",
    "  def __init__(self, X, Y, hp=\"max_depth\"):\n",
    "    \"\"\"Save training data\"\"\"\n",
    "    self.X = X    # features\n",
    "    self.Y = Y    # targets\n",
    "    self.hp = hp  # hyperparameter\n",
    "\n",
    "\n",
    "  def set_method(self, hp):\n",
    "    \"\"\"Set hyperparameter to use\"\"\"\n",
    "    self.hp = hp\n",
    "\n",
    "\n",
    "  def cross_me(self, *hp_vals):\n",
    "    \"\"\"Perform cross validation for given hyperparameter values\"\"\"\n",
    "    self.scores = []  # the accuracy table\n",
    "    self.best = None  # the best fit\n",
    "\n",
    "    best_score = 0\n",
    "\n",
    "    for hp in hp_vals:\n",
    "      # create a tree with given hyperparameter cut\n",
    "      fit = DecisionTreeRegressor(**{self.hp: hp})\n",
    "\n",
    "      # calculate a cross validation scores and a mean value\n",
    "      score = cross_val_score(fit, np.reshape(X, (-1, 1)), Y).mean()\n",
    "\n",
    "      # update best fit if necessary\n",
    "      if score > best_score:\n",
    "        self.best = fit\n",
    "        best_score = score\n",
    "\n",
    "      self.scores.append([hp, score])\n",
    "\n",
    "    # train the best fit\n",
    "    self.best.fit(np.reshape(X, (-1, 1)), Y)\n",
    "\n",
    "\n",
    "  def plot(self):\n",
    "    \"\"\"Plot accuracy as a function of hyperparameter values and best fit\"\"\"\n",
    "    plt.figure(figsize=(10, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "\n",
    "    plt.xlabel(self.hp)\n",
    "    plt.ylabel(\"accuracy\")\n",
    "\n",
    "    plt.plot(*zip(*self.scores))\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "\n",
    "    X_test = np.arange(0.0, 1.0, 0.01)[:, np.newaxis]\n",
    "    Y_test = self.best.predict(X_test)\n",
    "\n",
    "    plt.scatter(self.X, self.Y, color='b', marker='.', label=\"Training data\")\n",
    "    plt.plot(X_test, X_test * X_test, 'g', label=\"True distribution\")\n",
    "    plt.plot(X_test, Y_test, 'r', label=\"Decision tree\")\n",
    "\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nmi83jQaySdA"
   },
   "source": [
    "Let's generate some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 402,
     "status": "ok",
     "timestamp": 1705048107210,
     "user": {
      "displayName": "QILIN LI",
      "userId": "01132023268323719979"
     },
     "user_tz": -480
    },
    "id": "TkxXz4PMyVBm"
   },
   "outputs": [],
   "source": [
    "X = np.random.sample(200)\n",
    "Y = np.array([x**2 + np.random.normal(0, 0.05) for x in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Fx9knOjyWxK"
   },
   "source": [
    "And let's try to tune the hyperparameter ``max_depth``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "executionInfo": {
     "elapsed": 1722,
     "status": "ok",
     "timestamp": 1705048206773,
     "user": {
      "displayName": "QILIN LI",
      "userId": "01132023268323719979"
     },
     "user_tz": -480
    },
    "id": "pL0lGdm6ygV6",
    "outputId": "e6df044a-6d56-4e01-e951-a7cab4e686a4"
   },
   "outputs": [],
   "source": [
    "tree_handler = TreeCV(X, Y)\n",
    "tree_handler.cross_me(*range(1, 10))\n",
    "tree_handler.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEm5g2NlyzgP"
   },
   "source": [
    "As seen, we have examined the ``max_depth`` from 1 to 9, and the cross validation process indicates that ``max_depth==4`` might be a good choice. The plot on the right verifies it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9S27w1-zPjX"
   },
   "source": [
    "<a name=\"4.1\"></a>\n",
    "### 4.1 - Exercise03: hyperparameter tunning\n",
    "\n",
    "Following the above process, you can try to tune other hyperparameters using cross validation, such as\n",
    "\n",
    "\n",
    "*   ``min_samples_leaf``\n",
    "*   ``min_samples_split``\n",
    "*   ``min_impurity_decrease``\n",
    "\n",
    "\n",
    "\n",
    "You can find a list of hyperparameters from [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2D1YsH9rDWi"
   },
   "source": [
    "That's all. Well done!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOe1ZIAQEC2QIiYKck9LIxg",
   "provenance": [
    {
     "file_id": "1pBZPW8W-GeT-LJ6Ruav7N-DkowsS4nFG",
     "timestamp": 1705033585893
    },
    {
     "file_id": "1nGPchPxessD8iQ-SOcpBx-MG8xweGsv-",
     "timestamp": 1704790509697
    },
    {
     "file_id": "1d5urodzMUrldDaizDkoRggzF11RYCqdS",
     "timestamp": 1704777140011
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
